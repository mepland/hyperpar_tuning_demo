{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Demo\n",
    "### Matthew Epland, PhD\n",
    "Adapted from:\n",
    "* [Sklearn Documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html)\n",
    "* [yandexdataschool/mlhep2018 Slides](https://github.com/yandexdataschool/mlhep2018/blob/master/day4-Fri/Black-Box.pdf)\n",
    "* [Hyperparameter Optimization in Python Part 1: Scikit-Optimize](https://towardsdatascience.com/hyperparameter-optimization-in-python-part-1-scikit-optimize-754e485d24fe)\n",
    "* [An Introductory Example of Bayesian Optimization in Python with Hyperopt](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required packages via pip if necessary, only run if you know what you're doing! [Reference](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)  \n",
    "**Note: This does not use a virtual environment and will pip install directly to your system!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "# !{sys.executable} -m pip install -r gentun/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip uninstall --yes gentun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd gentun/\n",
    "# python3 setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many cores we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "########################################################\n",
    "# python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from time import time\n",
    "# from copy import copy\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from scipy.io import arff\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "########################################################\n",
    "# xgboost, sklearn\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore', message='sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23')\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "########################################################\n",
    "# skopt\n",
    "from skopt import Optimizer\n",
    "from skopt.learning import GaussianProcessRegressor, RandomForestRegressor, GradientBoostingQuantileRegressor\n",
    "from skopt.learning.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "########################################################\n",
    "# hyperopt\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "\n",
    "########################################################\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "########################################################\n",
    "# set global rnd_seed for reproducibility\n",
    "rnd_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * # load some helper functions, but keep main body of code in notebook for easier reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import * # load plotting code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = {\n",
    "    'RS': 500,\n",
    "     # 'GS': set by the size of the grid\n",
    "    'GP': 300,\n",
    "    'RF': 300,\n",
    "    'GBDT': 300,\n",
    "    'TPE': 300,\n",
    "    # 'GA': 300, # number of generations\n",
    "}\n",
    "\n",
    "# all will effectively be multiplied by n_folds\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for testing lower iterations and folds\n",
    "for k,v in n_iters.items():\n",
    "    n_iters[k] = 30\n",
    "\n",
    "# n_iters['GA'] = 1\n",
    "n_folds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to implement our own custom scorer to actually use the best number of trees found by early stopping.\n",
    "See the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_early_stopping_auc_scorer(model, X, y):\n",
    "    # predict_proba may not be thread safe, so copy the object - unfortunately getting crashes so just use the original object\n",
    "    # model = copy.copy(model_in)\n",
    "    y_pred = model.predict_proba(X, ntree_limit=model.best_ntree_limit)\n",
    "    y_pred_sig = y_pred[:,1]\n",
    "    return roc_auc_score(y, y_pred_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Polish Companies Bankruptcy Data\n",
    "### [Source and data dictionary](http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('./data/1year.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].apply(int, args=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real feature names, for reference\n",
    "with open ('./attrs.json') as json_file:\n",
    "    attrs_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Target and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='class'\n",
    "features = sorted(list(set(df.columns)-set([target])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Train, Validation, and Holdout Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "X_trainCV, X_holdout, y_trainCV, y_holdout = train_test_split(X, y, test_size=0.2, random_state=rnd_seed, stratify=y)\n",
    "del X; del y;\n",
    "\n",
    "dm_train = xgb.DMatrix(X_trainCV, label=y_trainCV)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainCV, y_trainCV, test_size=0.2, random_state=rnd_seed, stratify=y_trainCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Stratified k-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=rnd_seed+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Hyperparameter Search Space\n",
    "See the [docs here](https://xgboost.readthedocs.io/en/latest/parameter.html) for XGBoost hyperparameter details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = OrderedDict({\n",
    "    'max_depth': {'initial': 6, 'range': (3, 10), 'dist': randint(3, 10), 'grid': [4, 6, 8], 'hp': hp.choice('max_depth', [3,4,5,6,7,8,9,10])},\n",
    "        # default=6, Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
    "    'learning_rate': {'initial': 0.3, 'range': (0.05, 0.6), 'dist': uniform(0.05, 0.6), 'grid': [0.1, 0.15, 0.3], 'hp': hp.uniform('learning_rate', 0.05, 0.6)},\n",
    "        # NOTE: Optimizing the log of the learning rate would be better, but avoid that complexity for this demo...\n",
    "        # default=0.3, Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. alias: learning_rate\n",
    "    # 'min_child_weight': {'initial': 1., 'range': (1., 10.), 'dist': uniform(1., 10.), 'grid': [1., 3.], 'hp': hp.uniform('min_child_weight', 1., 10.)},\n",
    "        # default=1, Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "    'gamma': {'initial': 0., 'range': (0., 5.), 'dist': uniform(0., 5.), 'grid': [0., 0.5, 1.], 'hp': hp.uniform('gamma', 0., 5.)},\n",
    "        # default=0, Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. alias: min_split_loss\n",
    "    'reg_alpha': {'initial': 0., 'range': (0., 5.), 'dist': uniform(0., 5.), 'grid': [0., 1.], 'hp': hp.uniform('reg_alpha', 0., 5.)},\n",
    "        # default=0, L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    'reg_lambda': {'initial': 1., 'range': (0., 5.), 'dist': uniform(0., 5.), 'grid': [0., 1.], 'hp': hp.uniform('reg_lambda', 0., 5.)},\n",
    "        # default=1, L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    # 'max_delta_step': {'initial': 0., 'range': (0., 5.), 'dist': uniform(0., 5.), 'grid': [0., 1.], 'hp': hp.uniform('max_delta_step', 0., 5.)},\n",
    "        # default=0, Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.\n",
    "    # TODO debug ranges (0, 1) so they are actually working\n",
    "    # 'colsample_bytree': {'initial': 1., 'range': (0.5, 1.), 'dist': uniform(0.5, 1.), 'grid': [0.5, 1.], 'hp': hp.uniform('colsample_bytree', 0.5, 1.)},\n",
    "        # default=1, Subsample ratio of columns when constructing each tree.\n",
    "    # 'subsample': {'initial': 1., 'range': (0.5, 1.), 'dist': uniform(0.5, 1.), 'grid': [0.5, 1.], 'hp': hp.uniform('subsample', 0.5, 1.)},\n",
    "        # default=1, Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break out the params_to_be_opt, and their ranges (dimensions), and initial values\n",
    "params_to_be_opt = []\n",
    "dimensions = []\n",
    "for k,v in all_params.items():\n",
    "    params_to_be_opt.append(k)\n",
    "    dimensions.append(v['range'])\n",
    "\n",
    "# break out dictionaries for each optimizer\n",
    "params_initial = {}\n",
    "param_dists = {}\n",
    "param_grids = {}\n",
    "param_hp_dists = OrderedDict()\n",
    "for k,v in all_params.items():\n",
    "    params_initial[k] = v['initial']\n",
    "    param_dists[k] = v['dist']\n",
    "    param_grids[k] = v['grid']\n",
    "    param_hp_dists[k] = v['hp']\n",
    "\n",
    "# make helper param index dict\n",
    "param_index_dict = {}\n",
    "for iparam, param in enumerate(params_to_be_opt):\n",
    "    param_index_dict[param] = iparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set other fixed hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_setup_params = {\n",
    "'max_num_boost_rounds': 500, # maximum number of boosting rounds to run / trees to create\n",
    "'xgb_objective': 'binary:logistic', # objective function for binary classification\n",
    "'xgb_verbosity': 0, #  The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "'xgb_n_jobs': -1, # Number of parallel threads used to run XGBoost. -1 makes use of all cores in your system\n",
    "}\n",
    "\n",
    "# search_scoring = 'roc_auc' # need to use custom function to work properly with xgb early stopping, see xgb_early_stopping_auc_scorer\n",
    "search_n_jobs = -1 # Number of parallel threads used to run hyperparameter searches. -1 makes use of all cores in your system\n",
    "search_verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fit_params = {\n",
    "    'early_stopping_rounds': 10, # must see improvement over last num_early_stopping_rounds or will halt\n",
    "    'eval_set': [(X_val, y_val)], # data sets to use for early stopping evaluation\n",
    "    'eval_metric': 'auc', # evaluation metric for early stopping\n",
    "    'verbose': False, # even more verbosity control\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimators=fixed_setup_params['max_num_boost_rounds'],\n",
    "                              objective=fixed_setup_params['xgb_objective'],\n",
    "                              verbosity=fixed_setup_params['xgb_verbosity'],\n",
    "                              random_state=rnd_seed+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run with initial hyperparameters as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_initial = xgb.XGBClassifier(n_estimators=fixed_setup_params['max_num_boost_rounds'],\n",
    "                                  objective=fixed_setup_params['xgb_objective'],\n",
    "                                  verbosity=fixed_setup_params['xgb_verbosity'],\n",
    "                                  random_state=rnd_seed+3, **params_initial)\n",
    "model_initial.fit(X_train, y_train, **fixed_fit_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_initial = -xgb_early_stopping_auc_scorer(model_initial, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "Randomly test different hyperparameters drawn from `param_dists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dists, scoring=xgb_early_stopping_auc_scorer,\n",
    "                        n_iter=n_iters['RS'], n_jobs=search_n_jobs, cv=skf, verbose=search_verbosity, random_state=rnd_seed+4\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2500 out of 2500 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 298.69 seconds for 500 candidates parameter settings\n"
     ]
    }
   ],
   "source": [
    "rs_start = time()\n",
    "\n",
    "rs.fit(X_trainCV, y_trainCV, groups=None, **fixed_fit_params)\n",
    "dump_to_pkl(rs, 'RS')\n",
    "\n",
    "rs_time = time()-rs_start\n",
    "\n",
    "print(f\"RandomizedSearchCV took {rs_time:.2f} seconds for {n_iters['RS']} candidates parameter settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = load_from_pkl('RS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.942 (std: 0.016)\n",
      "Parameters: {'gamma': 1.251481577417663, 'learning_rate': 0.45272630810227477, 'max_depth': 5, 'reg_alpha': 0.5421290523363859, 'reg_lambda': 1.466492872089828}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.941 (std: 0.019)\n",
      "Parameters: {'gamma': 0.38392095253342695, 'learning_rate': 0.26832747636593524, 'max_depth': 8, 'reg_alpha': 0.7102822945994652, 'reg_lambda': 1.3093734188070028}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.941 (std: 0.017)\n",
      "Parameters: {'gamma': 1.4512726335156594, 'learning_rate': 0.19923692172418622, 'max_depth': 3, 'reg_alpha': 0.4303818282918964, 'reg_lambda': 1.2119491078724827}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sklearn_to_csv(rs, params_to_be_opt, tag='_RS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(y_values=np.array([-y for y in rs.cv_results_['mean_test_score']]), ann_text='RS', tag='_RS', y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Try all possible hyperparameter combinations `param_grids`, slow and poor exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=xgb_model, param_grid=param_grids, scoring=xgb_early_stopping_auc_scorer,\n",
    "                  n_jobs=search_n_jobs, cv=skf, verbose=search_verbosity # , iid=False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 97.18 seconds for 108 candidates parameter settings\n"
     ]
    }
   ],
   "source": [
    "gs_start = time()\n",
    "\n",
    "gs.fit(X_trainCV, y_trainCV, groups=None, **fixed_fit_params)\n",
    "dump_to_pkl(gs, 'GS')\n",
    "\n",
    "gs_time = time()-gs_start\n",
    "\n",
    "print(f\"GridSearchCV took {gs_time:.2f} seconds for {len(gs.cv_results_['params'])} candidates parameter settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = load_from_pkl('GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.943 (std: 0.020)\n",
      "Parameters: {'gamma': 0.5, 'learning_rate': 0.15, 'max_depth': 4, 'reg_alpha': 0.0, 'reg_lambda': 1.0}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.943 (std: 0.015)\n",
      "Parameters: {'gamma': 0.0, 'learning_rate': 0.3, 'max_depth': 4, 'reg_alpha': 0.0, 'reg_lambda': 1.0}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.943 (std: 0.019)\n",
      "Parameters: {'gamma': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sklearn_to_csv(gs, params_to_be_opt, tag='_GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(y_values=[-y for y in gs.cv_results_['mean_test_score']], ann_text='GS', tag='_GS', y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup datasets and objective function for custom searches"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# setup the function to be optimized - without CV\n",
    "def objective_function(params):\n",
    "    model = xgb.XGBClassifier(n_estimators=fixed_setup_params['max_num_boost_rounds'], objective=fixed_setup_params['xgb_objective'], verbosity=fixed_setup_params['xgb_verbosity'], random_state=rnd_seed+6, **params)\n",
    "    model.fit(X_train_OPT, y_train_OPT, early_stopping_rounds=fixed_fit_params['early_stopping_rounds'], eval_set=[(X_val_OPT, y_val_OPT)], eval_metric=fixed_fit_params['eval_metric'], verbose=fixed_fit_params['verbose'])\n",
    "\n",
    "    best_ntree_limit = model.best_ntree_limit\n",
    "    if best_ntree_limit >= fixed_setup_params['max_num_boost_rounds']:\n",
    "        print(f\"Hit max_num_boost_rounds = {fixed_setup_params['max_num_boost_rounds']:d}, model.best_ntree_limit = {best_ntree_limit:d}\")\n",
    "\n",
    "    # return the negative auc of the trained model, since Optimizer and hyperopt only minimize\n",
    "    return -xgb_early_stopping_auc_scorer(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the function to be optimized\n",
    "def objective_function(params):\n",
    "    cv = xgb.cv(dict({'objective': fixed_setup_params['xgb_objective']}, **params), dm_train,\n",
    "                num_boost_round=fixed_setup_params['max_num_boost_rounds'], early_stopping_rounds=fixed_fit_params['early_stopping_rounds'],\n",
    "                nfold=n_folds, stratified=True, folds=skf,\n",
    "                metrics=fixed_fit_params['eval_metric'],\n",
    "                verbose_eval=fixed_fit_params['verbose'], seed=rnd_seed+6, as_pandas=True)\n",
    "\n",
    "    # return the negative auc of the trained model, since Optimizer and hyperopt only minimize\n",
    "    return -cv[f\"test-{fixed_fit_params['eval_metric']}-mean\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "Use Bayesian optimization to intelligently decide where to sample the objective function next, based on prior results.  \n",
    "Can use many different types of surrogate functions: Gaussian Process, Random Forest, Gradient Boosted Trees. Note that the Tree-Structured Parzen Estimator (TPE) approach is a close cousin of Bayesian optimization, similar in operation but arising from a flipped form of Bayes rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_initial_points = 0.1\n",
    "acq_func='gp_hedge' # select the best of EI, PI, LCB per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bo(bo_opt, bo_n_iter, ann_text, m_path='output', tag='', params_initial=None, y_initial=None, print_interval=5, debug=False):\n",
    "    iter_results = []\n",
    "    if params_initial is not None and y_initial is not None:\n",
    "        # update bo_opt with the initial point, might as well since we have already computed it!\n",
    "        x_initial = [params_initial[param] for param in params_to_be_opt]\n",
    "        bo_opt.tell(x_initial, y_initial)\n",
    "\n",
    "        initial_row = {'iter':0, 'y':y_initial, 'auc':-y_initial}\n",
    "        for param in params_to_be_opt:\n",
    "            initial_row[param] = params_initial[param]\n",
    "        iter_results.append(initial_row)\n",
    "\n",
    "    # we'll print these warnings ourselves\n",
    "    warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "\n",
    "    # run it\n",
    "    for i_iter in range(1,bo_n_iter):\n",
    "        is_last = False\n",
    "        if i_iter+1 == bo_n_iter:\n",
    "            is_last = True\n",
    "\n",
    "        print_this_i = False\n",
    "        if is_last or (print_interval !=0 and (print_interval < 0 or (print_interval > 0 and i_iter % print_interval == 0))):\n",
    "            print_this_i = True\n",
    "            print(f'Starting iteration {i_iter:d}')\n",
    "\n",
    "        # get next test point x, ie a new point beta in parameter space\n",
    "        x = bo_opt.ask()\n",
    "\n",
    "        is_repeat = False\n",
    "        if x in bo_opt.Xi:\n",
    "            is_repeat = True\n",
    "            # we have already evaluated objective_function at this point! Pull old value, give it back and continue.\n",
    "            # not very elegant, might still get a warning from Optimizer, but at least is MUCH faster than recomputing objective_function...\n",
    "            past_i_iter = bo_opt.Xi.index(x)\n",
    "            y = bo_opt.yi[past_i_iter] # get from bo_opt array to be sure it's the right one\n",
    "\n",
    "            if debug:\n",
    "                print('Already evaluated at this x (params below)! Just using the old result for y and continuing!')\n",
    "                print(x)\n",
    "\n",
    "        else:\n",
    "            # run the training and predictions for the test point\n",
    "            params = {}\n",
    "            for param,value in zip(params_to_be_opt,x):\n",
    "                params[param] = value\n",
    "            y = objective_function(params)\n",
    "        # update bo_opt with the result for the test point\n",
    "        bo_opt.tell(x, y)\n",
    "\n",
    "        # save to df\n",
    "        iter_result = {'iter':i_iter, 'y':y, 'auc':-y} # , 'x': str(x)\n",
    "        for param,value in zip(params_to_be_opt,x):\n",
    "            iter_result[param] = value\n",
    "        iter_results.append(iter_result)\n",
    "\n",
    "        # see if it is a min\n",
    "        is_best = False\n",
    "        if i_iter != 0 and y == np.array(bo_opt.yi).min():\n",
    "            is_best = True\n",
    "\n",
    "        # print messages and plots while still running\n",
    "        if print_this_i or is_last or (is_best and not is_repeat):\n",
    "            if is_best:\n",
    "                print('Found a new optimum set of hyper parameters!')\n",
    "            print(x)\n",
    "            print(f'y: {y:.5f}')\n",
    "\n",
    "            df_tmp = pd.DataFrame.from_dict(iter_results)\n",
    "\n",
    "            plot_convergence(df_tmp['y'], ann_text, m_path, tag=tag, y_initial=y_initial)\n",
    "\n",
    "    # save iter results to csv\n",
    "    df_iter_results = pd.DataFrame.from_dict(iter_results)\n",
    "    df_iter_results = df_iter_results.sort_values(by='iter').reset_index(drop=True)\n",
    "    fixed_cols = ['iter', 'y', 'auc']+params_to_be_opt\n",
    "    cols = fixed_cols + list(set(df_iter_results.columns)-set(fixed_cols))\n",
    "    df_iter_results = df_iter_results[cols]\n",
    "    df_iter_results.to_csv(f'{m_path}/iter_results{tag}.csv', index=False, na_rep='nan')\n",
    "\n",
    "    # save best results to csv\n",
    "    y_best = df_iter_results['y'].min()\n",
    "    df_best = df_iter_results.loc[y_best == df_iter_results['y']]\n",
    "    df_best = df_best.drop_duplicates(subset=params_to_be_opt).reset_index(drop=True)\n",
    "    df_best = df_best.sort_values(by=params_to_be_opt).reset_index(drop=True)\n",
    "    df_best = df_best[['y', 'auc']+params_to_be_opt]\n",
    "    df_best.to_csv(f'{m_path}/best_params_points{tag}.csv', index=False, na_rep='nan')\n",
    "\n",
    "    n_best_params_points = len(df_best)\n",
    "\n",
    "    print('\\nBest hyperparameters:')\n",
    "    for index,row in df_best.iterrows():\n",
    "        if n_best_params_points > 1:\n",
    "            print(f'\\nPoint {index}:')\n",
    "\n",
    "        for param in params_to_be_opt:\n",
    "            print(f'{param}: {row[param]}')\n",
    "\n",
    "    if n_best_params_points > 1:\n",
    "        print('Parameter Mean and St Dev:')\n",
    "        for param in params_to_be_opt:\n",
    "            param_values = df_best[param].values\n",
    "            print(f'{param} Mean: {np.mean(param_values)}, St Dev: {np.std(param_values)}')\n",
    "\n",
    "    if y_initial is not None:\n",
    "        print(f'\\n   Best y: {y_best:.5f} (should be smaller than initial), a decrease of {y_initial-y_best:.5f}, {(y_initial-y_best)/y_initial:.3%}')\n",
    "        print(f'Initial y: {y_initial:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radial basis function + white noise kernel\n",
    "bo_gp_opt = Optimizer(dimensions=dimensions, n_initial_points=np.ceil(frac_initial_points*n_iters['GP']), acq_func=acq_func, random_state=rnd_seed+6,\n",
    "                      base_estimator=GaussianProcessRegressor(\n",
    "                          kernel=RBF(length_scale_bounds=[1.0e-3, 1.0e+3]) + WhiteKernel(noise_level=1.0e-5, noise_level_bounds=[1.0e-6, 1.0e-2])\n",
    "                      ),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 50\n",
      "[3, 0.21905078811439793, 0.0706322697466122, 0.4748809506842395, 1.42628222182151]\n",
      "y: -0.93928\n",
      "Starting iteration 100\n",
      "[10, 0.055006779985515876, 0.011424231255486307, 0.46610677443296006, 4.99796588857946]\n",
      "y: -0.93864\n",
      "Found a new optimum set of hyper parameters!\n",
      "[3, 0.16271813800018722, 0.0, 0.0, 0.0]\n",
      "y: -0.94708\n",
      "Found a new optimum set of hyper parameters!\n",
      "[3, 0.15690938149876643, 0.0, 0.0, 0.0]\n",
      "y: -0.94893\n",
      "Starting iteration 150\n",
      "[3, 0.14742557688869995, 0.0, 0.0, 0.0]\n",
      "y: -0.93978\n",
      "Starting iteration 200\n",
      "[3, 0.24487667021602094, 0.6911132512716601, 0.1662366112659092, 0.661005980393451]\n",
      "y: -0.94420\n",
      "Starting iteration 250\n",
      "[3, 0.09860821336347633, 1.3375607667664389, 0.04928129573143148, 0.40401622672261]\n",
      "y: -0.94097\n",
      "Starting iteration 299\n",
      "Found a new optimum set of hyper parameters!\n",
      "[3, 0.13859438857687764, 0.7262535985191189, 0.23929680091505928, 0.03487494663961533]\n",
      "y: -0.94934\n",
      "\n",
      "Best hyperparameters:\n",
      "max_depth: 3.0\n",
      "learning_rate: 0.13859438857687764\n",
      "gamma: 0.7262535985191189\n",
      "reg_alpha: 0.23929680091505928\n",
      "reg_lambda: 0.03487494663961533\n",
      "\n",
      "   Best y: -0.94934 (should be smaller than initial), a decrease of 0.00359, -0.380%\n",
      "Initial y: -0.94575\n"
     ]
    }
   ],
   "source": [
    "run_bo(bo_gp_opt, bo_n_iter=n_iters['GP'], ann_text='GP', tag='_GP', params_initial=params_initial, y_initial=y_initial, print_interval=50)\n",
    "dump_to_pkl(bo_gp_opt, 'GP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_gp_opt = load_from_pkl('GP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_opt = Optimizer(dimensions=dimensions, n_initial_points=np.ceil(frac_initial_points*n_iters['RF']), acq_func=acq_func, random_state=rnd_seed+7,\n",
    "                      base_estimator=RandomForestRegressor(n_estimators=200, max_depth=8, random_state=rnd_seed+8),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 50\n",
      "[6, 0.07104191864537288, 1.4274746214002167, 0.8619738846034087, 0.8738030392141989]\n",
      "y: -0.93952\n",
      "Starting iteration 100\n",
      "[7, 0.05317702947609369, 2.3659239592241708, 2.9830266159483316, 3.898691836027124]\n",
      "y: -0.92993\n",
      "Found a new optimum set of hyper parameters!\n",
      "[5, 0.229848817811993, 0.12765928800300255, 0.04453222707426375, 0.08041453058157734]\n",
      "y: -0.94740\n",
      "Starting iteration 150\n",
      "[5, 0.05059886961899329, 2.379098892388575, 0.5337004390534557, 0.22852657344694052]\n",
      "y: -0.93563\n",
      "Found a new optimum set of hyper parameters!\n",
      "[4, 0.3217115619095135, 0.09879551452404768, 0.16011533227481115, 0.0826289293890853]\n",
      "y: -0.94844\n",
      "Starting iteration 200\n",
      "[4, 0.05319686419446629, 2.107540171337259, 0.4730696272680519, 3.3686720086766426]\n",
      "y: -0.93876\n",
      "Starting iteration 250\n",
      "[10, 0.07314875571334944, 0.38989578795753993, 0.08782336547812687, 0.967023303220009]\n",
      "y: -0.91382\n",
      "Starting iteration 299\n",
      "[6, 0.0510786301738719, 1.8932503446014446, 3.844665704435739, 0.016539640999839737]\n",
      "y: -0.93176\n",
      "\n",
      "Best hyperparameters:\n",
      "max_depth: 4.0\n",
      "learning_rate: 0.3217115619095135\n",
      "gamma: 0.09879551452404768\n",
      "reg_alpha: 0.16011533227481115\n",
      "reg_lambda: 0.0826289293890853\n",
      "\n",
      "   Best y: -0.94844 (should be smaller than initial), a decrease of 0.00269, -0.284%\n",
      "Initial y: -0.94575\n"
     ]
    }
   ],
   "source": [
    "run_bo(bo_rf_opt, bo_n_iter=n_iters['RF'], ann_text='RF', tag='_RF', params_initial=params_initial, y_initial=y_initial, print_interval=50)\n",
    "dump_to_pkl(bo_rf_opt, 'RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_rf_opt = load_from_pkl('RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_base_estimator = GradientBoostingQuantileRegressor(\n",
    "    base_estimator=GradientBoostingRegressor(loss='quantile', max_depth=8, learning_rate=0.1, n_estimators=200,\n",
    "                                             n_iter_no_change=10, validation_fraction=0.2, tol=0.0001, random_state=rnd_seed+9)\n",
    ")\n",
    "\n",
    "bo_gbdt_opt = Optimizer(dimensions=dimensions, n_initial_points=np.ceil(frac_initial_points*n_iters['GBDT']), acq_func=acq_func,\n",
    "                        random_state=rnd_seed+10, base_estimator=gbrt_base_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 50\n",
      "[4, 0.06837161680291476, 0.022986788127408734, 3.480074241005479, 1.6376193074787768]\n",
      "y: -0.93685\n",
      "Starting iteration 100\n",
      "[3, 0.1769795863337964, 0.46702237156056875, 1.3623279205959873, 0.6018708070323837]\n",
      "y: -0.93875\n",
      "Found a new optimum set of hyper parameters!\n",
      "[4, 0.18586436207953033, 0.5574452480299276, 0.08880771088900065, 0.007518349302936269]\n",
      "y: -0.94600\n",
      "Starting iteration 150\n",
      "[10, 0.05497400936313704, 0.7289277239153309, 0.12247196788701678, 0.7742293022832787]\n",
      "y: -0.91072\n",
      "Starting iteration 200\n",
      "[8, 0.11425393226927932, 4.989382011775662, 0.39934938599035524, 1.687554235002666]\n",
      "y: -0.93077\n",
      "Starting iteration 250\n",
      "[6, 0.08586281740012999, 0.024895600541173794, 0.11366279176438733, 1.894110739107912]\n",
      "y: -0.94520\n",
      "Starting iteration 299\n",
      "[10, 0.5639526027140095, 3.3553899159322644, 0.5396739520808093, 2.457053815598034]\n",
      "y: -0.92027\n",
      "\n",
      "Best hyperparameters:\n",
      "max_depth: 4.0\n",
      "learning_rate: 0.18586436207953033\n",
      "gamma: 0.5574452480299276\n",
      "reg_alpha: 0.08880771088900065\n",
      "reg_lambda: 0.007518349302936269\n",
      "\n",
      "   Best y: -0.94600 (should be smaller than initial), a decrease of 0.00025, -0.027%\n",
      "Initial y: -0.94575\n"
     ]
    }
   ],
   "source": [
    "run_bo(bo_gbdt_opt, bo_n_iter=n_iters['GBDT'], ann_text='GBDT', tag='_GBDT', params_initial=params_initial, y_initial=y_initial, print_interval=50)\n",
    "dump_to_pkl(bo_gbdt_opt, 'GBDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_gbdt_opt = load_from_pkl('GBDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Structured Parzen Estimator (TPE)\n",
    "Note that hyperopt with TPE can accommodate nested hyperparameter search distributions. See [here](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a#951b) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [1:09:02<00:00, 13.81s/it, best loss: -0.9459936000000001]\n"
     ]
    }
   ],
   "source": [
    "tpe_trials = Trials()\n",
    "\n",
    "tpe_best = fmin(fn=objective_function, space=param_hp_dists, algo=tpe.suggest, max_evals=n_iters['TPE'],\n",
    "                trials=tpe_trials, rstate=np.random.RandomState(rnd_seed+11))\n",
    "dump_to_pkl(tpe_trials, 'TPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_trials = load_from_pkl('TPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(y_values=tpe_trials.losses(), ann_text='TPE', tag='_TPE', y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hyperopt_to_csv(tpe_trials, params_to_be_opt, tag='_TPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eventual TODOs\n",
    "* Multithreading\n",
    "* Set random seed, but would require a careful rewrite of gentun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gentun import GeneticAlgorithm, Population, GridPopulation, XgboostIndividual"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Generate a grid of individuals as the initial population\n",
    "# Use the same grid as in the sklearn grid search, and the first generation will be the same as that grid search\n",
    "# large computation, requires working multithreading / distributed computing\n",
    "pop = GridPopulation(XgboostIndividual, X_trainCV, y_trainCV, genes_grid=param_grids,\n",
    "                     additional_parameters={'kfold': n_folds,\n",
    "                                            'objective': fixed_setup_params['xgb_objective'],\n",
    "                                            'eval_metric': fixed_fit_params['eval_metric'],\n",
    "                                            'num_boost_round': fixed_setup_params['max_num_boost_rounds'],\n",
    "                                            'early_stopping_rounds': fixed_fit_params['early_stopping_rounds'],\n",
    "                                            'folds': skf, # stratified kfolds from sklearn\n",
    "                                            'verbose_eval': fixed_fit_params['verbose'],\n",
    "                                           },\n",
    "                     crossover_rate=0.5, mutation_rate=0.02, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a random population. Size: 25\n"
     ]
    }
   ],
   "source": [
    "# proof of concept code, turn off kfolds, run a low number of iterations\n",
    "# Generate a random sample of 25 individuals as the initial population\n",
    "pop = Population(XgboostIndividual, X_trainCV, y_trainCV,\n",
    "                 size=25,\n",
    "                 additional_parameters={# 'kfold': n_folds,\n",
    "                                        'objective': fixed_setup_params['xgb_objective'],\n",
    "                                        'eval_metric': fixed_fit_params['eval_metric'],\n",
    "                                        'num_boost_round': fixed_setup_params['max_num_boost_rounds'],\n",
    "                                        'early_stopping_rounds': fixed_fit_params['early_stopping_rounds'],\n",
    "                                        # 'folds': skf, # stratified kfolds from sklearn\n",
    "                                        'verbose_eval': fixed_fit_params['verbose'],\n",
    "                                       },\n",
    "                 crossover_rate=0.5, mutation_rate=0.05, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = GeneticAlgorithm(pop, tournament_size=5, elitism=True, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters['GA'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting genetic algorithm...\n",
      "\n",
      "Evaluating generation #1...\n",
      "Evaluating generation #2...\n",
      "Evaluating generation #3...\n",
      "Evaluating generation #4...\n",
      "Evaluating generation #5...\n",
      "Evaluating generation #6...\n",
      "Evaluating generation #7...\n",
      "Evaluating generation #8...\n",
      "Evaluating generation #9...\n",
      "Evaluating generation #10...\n"
     ]
    }
   ],
   "source": [
    "ga.run(n_iters['GA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_results = ga.get_results()\n",
    "ga_results = ga_results[['generation', 'best_fitness']+params_to_be_opt]\n",
    "dump_to_pkl(ga_results, 'GA') # is just a df, but might as well still pkl to be consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_results = load_from_pkl('GA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(y_values=np.array([-y for y in ga_results['best_fitness'].to_list()]), ann_text='GA', tag='_GA', y_initial=y_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gentun_to_csv(ga_results, params_to_be_opt, tag='_GA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance\n",
    "### Make evaluation and objective (when possible) plots from skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations((rs, param_hp_dists), ann_text='RS', tag='_RS', bins=10, dimensions=params_to_be_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations((gs, param_hp_dists), ann_text='GS', tag='_GS', bins=10, dimensions=params_to_be_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations(bo_gp_opt, ann_text='GP', tag='_GP', bins=10, dimensions=params_to_be_opt)\n",
    "my_plot_objective(bo_gp_opt, ann_text='GP', tag='_GP', dimensions=params_to_be_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations(bo_rf_opt, ann_text='RF', tag='_RF', bins=10, dimensions=params_to_be_opt)\n",
    "my_plot_objective(bo_rf_opt, ann_text='RF', tag='_RF', dimensions=params_to_be_opt) # takes much longer for these partial dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations(bo_gbdt_opt, ann_text='GBDT', tag='_GBDT', bins=10, dimensions=params_to_be_opt)\n",
    "my_plot_objective(bo_gbdt_opt, ann_text='GBDT', tag='_GBDT', dimensions=params_to_be_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations((tpe_trials, param_hp_dists), ann_text='TPE', tag='_TPE', bins=10, dimensions=params_to_be_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_evaluations((ga_results, param_hp_dists), ann_text='GA', tag='_GA', bins=10, dimensions=params_to_be_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best parameters from all optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_abbrevs = ['RS', 'GS', 'GP', 'RF', 'GBDT', 'TPE', 'GA']\n",
    "\n",
    "df_best_results = combine_best_results(optimizer_abbrevs, params_to_be_opt, params_initial, y_initial, m_path='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimizer</th>\n",
       "      <th>y</th>\n",
       "      <th>per_change</th>\n",
       "      <th>auc</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>gamma</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GA</td>\n",
       "      <td>-0.953364</td>\n",
       "      <td>-0.804952</td>\n",
       "      <td>0.953364</td>\n",
       "      <td>5</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GP</td>\n",
       "      <td>-0.949342</td>\n",
       "      <td>-0.379681</td>\n",
       "      <td>0.949342</td>\n",
       "      <td>3</td>\n",
       "      <td>0.138594</td>\n",
       "      <td>0.726254</td>\n",
       "      <td>0.239297</td>\n",
       "      <td>0.034875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RF</td>\n",
       "      <td>-0.948440</td>\n",
       "      <td>-0.284371</td>\n",
       "      <td>0.948440</td>\n",
       "      <td>4</td>\n",
       "      <td>0.321712</td>\n",
       "      <td>0.098796</td>\n",
       "      <td>0.160115</td>\n",
       "      <td>0.082629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GBDT</td>\n",
       "      <td>-0.946002</td>\n",
       "      <td>-0.026607</td>\n",
       "      <td>0.946002</td>\n",
       "      <td>4</td>\n",
       "      <td>0.185864</td>\n",
       "      <td>0.557445</td>\n",
       "      <td>0.088808</td>\n",
       "      <td>0.007518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>TPE</td>\n",
       "      <td>-0.945994</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>0.945994</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135612</td>\n",
       "      <td>1.972369</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>1.214715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Initial</td>\n",
       "      <td>-0.945751</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.945751</td>\n",
       "      <td>6</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>GS</td>\n",
       "      <td>-0.943294</td>\n",
       "      <td>0.259779</td>\n",
       "      <td>0.943294</td>\n",
       "      <td>4</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>RS</td>\n",
       "      <td>-0.941950</td>\n",
       "      <td>0.401842</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>5</td>\n",
       "      <td>0.452726</td>\n",
       "      <td>1.251482</td>\n",
       "      <td>0.542129</td>\n",
       "      <td>1.466493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  optimizer         y  per_change       auc  max_depth  learning_rate  \\\n",
       "0        GA -0.953364   -0.804952  0.953364          5       0.181400   \n",
       "1        GP -0.949342   -0.379681  0.949342          3       0.138594   \n",
       "2        RF -0.948440   -0.284371  0.948440          4       0.321712   \n",
       "3      GBDT -0.946002   -0.026607  0.946002          4       0.185864   \n",
       "4       TPE -0.945994   -0.025677  0.945994          0       0.135612   \n",
       "5   Initial -0.945751   -0.000000  0.945751          6       0.300000   \n",
       "6        GS -0.943294    0.259779  0.943294          4       0.150000   \n",
       "7        RS -0.941950    0.401842  0.941950          5       0.452726   \n",
       "\n",
       "      gamma  reg_alpha  reg_lambda  \n",
       "0  0.000200   0.000000    0.356300  \n",
       "1  0.726254   0.239297    0.034875  \n",
       "2  0.098796   0.160115    0.082629  \n",
       "3  0.557445   0.088808    0.007518  \n",
       "4  1.972369   0.002436    1.214715  \n",
       "5  0.000000   0.000000    1.000000  \n",
       "6  0.500000   0.000000    1.000000  \n",
       "7  1.251482   0.542129    1.466493  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_results.to_csv('./output/best_results_all.csv', index=False, na_rep='nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models with the best parameters from each optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_best_models(df_best_results):\n",
    "    best_models = {}\n",
    "    for index, row in df_best_results.iterrows():\n",
    "        params = {param: row[param] for k in params_to_be_opt}\n",
    "\n",
    "        this_model = xgb.XGBClassifier(n_estimators=fixed_setup_params['max_num_boost_rounds'],\n",
    "                                       objective=fixed_setup_params['xgb_objective'],\n",
    "                                       verbosity=fixed_setup_params['xgb_verbosity'],\n",
    "                                       random_state=rnd_seed+12, **params)\n",
    "        this_model.fit(X_train, y_train, **fixed_fit_params)\n",
    "\n",
    "        y_holdout_pred = this_model.predict_proba(X_holdout, ntree_limit=this_model.best_ntree_limit)[:,1]\n",
    "\n",
    "        fpr, tpr, thr = roc_curve(y_holdout, y_holdout_pred)\n",
    "\n",
    "        best_models[row['optimizer']] = dict({'model': this_model, 'y_holdout_pred': y_holdout_pred, 'fpr': fpr, 'tpr': tpr, 'thr': thr}, **row)\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = eval_best_models(df_best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_for_roc= [\n",
    "    {'name': 'Initial', 'nname': 'Initial', 'fpr': best_models['Initial']['fpr'], 'tpr': best_models['Initial']['tpr'], 'c': 'black', 'ls': '-'},\n",
    "    {'name': 'RS', 'nname': 'RS', 'fpr': best_models['RS']['fpr'], 'tpr': best_models['RS']['tpr'], 'c': 'C0', 'ls': ':'},\n",
    "    {'name': 'GS', 'nname': 'GS', 'fpr': best_models['GS']['fpr'], 'tpr': best_models['GS']['tpr'], 'c': 'C1', 'ls': '-.'},\n",
    "    {'name': 'GP', 'nname': 'GP', 'fpr': best_models['GP']['fpr'], 'tpr': best_models['GP']['tpr'], 'c': 'C2', 'ls': '-'},\n",
    "    {'name': 'RF', 'nname': 'RF', 'fpr': best_models['RF']['fpr'], 'tpr': best_models['RF']['tpr'], 'c': 'C3', 'ls': ':'},\n",
    "    {'name': 'GBDT', 'nname': 'GBDT', 'fpr': best_models['GBDT']['fpr'], 'tpr': best_models['GBDT']['tpr'], 'c': 'C4', 'ls': '--'},\n",
    "    {'name': 'TPE', 'nname': 'TPE', 'fpr': best_models['TPE']['fpr'], 'tpr': best_models['TPE']['tpr'], 'c': 'C5', 'ls': '-.'},\n",
    "    {'name': 'GA', 'nname': 'GA', 'fpr': best_models['GA']['fpr'], 'tpr': best_models['GA']['tpr'], 'c': 'C6', 'ls': '--'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rocs(models_for_roc, rndGuess=True, inverse_log=False, inline=False)\n",
    "plot_rocs(models_for_roc, rndGuess=False, inverse_log=True, tag='_inverse_log', x_axis_params={'max':0.4}, y_axis_params={'max':1e1}, inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot $\\hat{y}$ predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in best_models.items():\n",
    "    plot_y_pred(v['y_holdout_pred'], y_holdout, tag=f'_{k}', ann_text=k, nbins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('Stop Here, in Dev!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
