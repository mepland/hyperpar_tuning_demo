{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Demo\n",
    "### Matthew Epland\n",
    "Partially adapted from the [sklearn documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import arff\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "rnd_seed = 11\n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores from sklearn searches\n",
    "def report(results, n_top=5):\n",
    "    results = results.cv_results_\n",
    "    for i in range(1, n_top+1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(f'Model with rank: {i}')\n",
    "            print(f\"Mean validation score: {results['mean_test_score'][candidate]:.3f} (std: {results['std_test_score'][candidate]:.3f})\")\n",
    "            print('Parameters: {0}\\n'.format(results['params'][candidate]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to implement our own custom scorer to actually use the best number of trees found by early stopping.\n",
    "See the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_early_stopping_auc_scorer(model, X, y):\n",
    "    y_pred = model.predict_proba(X, ntree_limit=model.best_ntree_limit)\n",
    "    y_pred_sig = y_pred[:,1]\n",
    "    return roc_auc_score(y, y_pred_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Polish Companies Bankruptcy Data\n",
    "#### [Source and data dictionary](http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('./data/1year.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "df['class'] = df['class'].apply(int, args=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real feature names\n",
    "with open ('./attrs.json') as json_file:\n",
    "    attrs_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Target and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='class'\n",
    "features = sorted(list(set(df.columns)-set([target])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=0.2, random_state=rnd_seed, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=rnd_seed+1, stratify=y_tmp)\n",
    "del X_tmp; del y_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Stratified k-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=rnd_seed+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter Rangess\n",
    "#### See the [docs here](https://xgboost.readthedocs.io/en/latest/parameter.html) for XGBoost hyperparameter details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_boost_rounds = 200 # maximum number of boosting rounds to run / trees to create\n",
    "num_early_stopping_rounds = 10 # must see improvement over last num_early_stopping_rounds or will halt\n",
    "xgb_objective = 'binary:logistic'\n",
    "xgb_verbosity = 0 #  The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "xgb_n_jobs = 1 # Number of parallel threads used to run XGBoost. -1 makes use of all cores in your system\n",
    "# search_scoring = 'roc_auc' # need to use custom function to work properly with xgb early stopping, see xgb_early_stopping_auc_scorer\n",
    "search_n_jobs = 2 # Number of parallel threads used to run hyperparameter searches\n",
    "search_verbosity = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fit_params = {\n",
    "    'early_stopping_rounds': num_early_stopping_rounds,\n",
    "    'eval_set': [(X_val, y_val)],\n",
    "    'eval_metric': 'logloss',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dists = {\n",
    "    'max_depth': randint(3, 10), # default=6, Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
    "    'min_child_weight': uniform(1.0, 5.0), # default=1, Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "    'learning_rate': uniform(0.05, 1.0), # default=0.3, Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative. alias: learning_rate\n",
    "    # 'gamma': uniform(1, 5), # default=0, Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. alias: min_split_loss\n",
    "    # 'colsample_bytree': uniform(0.5, 1.0), # default=1, Subsample ratio of columns when constructing each tree.\n",
    "    # 'subsample': uniform(0.5, 1.0), # default=1, Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "    # 'max_delta_step': uniform(0.0, 5.0), # default=0, Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.\n",
    "    # 'reg_alpha': uniform(0.0, 5.0), # default=0, L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    # 'reg_lambda': uniform(0.0, 5.0), # default=1, L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_n_iter=50\n",
    "model_rs = xgb.XGBClassifier(n_estimators=max_num_boost_rounds, objective=xgb_objective, verbosity=xgb_verbosity)\n",
    "\n",
    "rs = RandomizedSearchCV(estimator=model_rs, param_distributions=param_dists, scoring=xgb_early_stopping_auc_scorer,\n",
    "                        n_iter=search_n_iter, n_jobs=search_n_jobs, cv=skf, verbose=search_verbosity, random_state=rnd_seed+3\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_start = time()\n",
    "\n",
    "rs.fit(X_train, y_train, groups=None, **fixed_fit_params)\n",
    "\n",
    "rs_time = time()-rs_start\n",
    "\n",
    "print(f'RandomizedSearchCV took {rs_time:.2f} seconds for {search_n_iter} candidates parameter settings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search time, best model, best params, for later comparison\n",
    "# rs_time\n",
    "# rs.best_estimator_\n",
    "# rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand alone\n",
    "model = xgb.XGBClassifier(max_depth=6, verbosity=0)\n",
    "model.fit(X_train, y_train, early_stopping_rounds=num_early_stopping_rounds, eval_set=[(X_val, y_val)], eval_metric='logloss')\n",
    "y_test_pred = model.predict_proba(X_test, ntree_limit=model.best_ntree_limit)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best from rs\n",
    "model = rs.best_estimator_\n",
    "y_test_pred = model.predict_proba(X_test, ntree_limit=model.best_ntree_limit)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_pred(y_pred, y):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sig_mask = np.where(y == 1)\n",
    "    bkg_mask = np.where(y == 0)\n",
    "\n",
    "    plt.hist(y_pred[sig_mask], bins=np.linspace(0,1,11), histtype='step', color='C0', label='Signal')\n",
    "    plt.hist(y_pred[bkg_mask], bins=np.linspace(0,1,11), histtype='step', color='C1', label='Background')\n",
    "\n",
    "    leg = ax.legend(loc='upper right',frameon=False)\n",
    "    leg.get_frame().set_facecolor('none')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.set_xlabel('$\\hat{y}$')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_xlim([0.,1.])\n",
    "\n",
    "    plt.show()\n",
    "    # fig.savefig('roc.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_pred(y_test_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thr = roc_curve(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, rndGuess=True, grid=False, better_ann=True):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    label=f'AUC {auc(fpr,tpr):.4f}'\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2, c='C0', ls='-', label=label)\n",
    "\n",
    "    if rndGuess:\n",
    "        x = np.linspace(0., 1.)\n",
    "        ax.plot(x, x, color='grey', linestyle=':', linewidth=2, label='Random Guess')\n",
    "\n",
    "    if grid:\n",
    "        ax.grid()\n",
    "\n",
    "    leg = ax.legend(loc='lower right',frameon=False)\n",
    "    leg.get_frame().set_facecolor('none')\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_xlim([0.,1.])\n",
    "    ax.set_ylim([0.,1.])\n",
    "\n",
    "    if better_ann:\n",
    "        plt.text(-0.08, 1.08, 'Better', size=12, rotation=45, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='green', alpha=0.2))\n",
    "\n",
    "    plt.show()\n",
    "    # fig.savefig('roc.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
